{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhM-ngugyMOP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "def generate_fake_data(n_rows=200, seed=42, out_path=\"simulated_data.csv\"):\n",
        "    \"\"\"\n",
        "    Create a DataFrame with two columns:\n",
        "      - 'text': a short fake sentence\n",
        "      - 'score': integer score from 0 to 100\n",
        "    and export it to `out_path` as CSV.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    subjects = [\n",
        "        \"The product\", \"Our team\", \"This model\", \"The feature\", \"The service\",\n",
        "        \"Customer feedback\", \"The interface\", \"The campaign\", \"The dataset\", \"The update\"\n",
        "    ]\n",
        "    verbs = [\n",
        "        \"significantly improves\", \"barely changes\", \"often breaks\", \"consistently boosts\",\n",
        "        \"slightly harms\", \"greatly simplifies\", \"occasionally confuses\",\n",
        "        \"reliably accelerates\", \"sometimes delays\", \"clearly enhances\"\n",
        "    ]\n",
        "    objects = [\n",
        "        \"user satisfaction\", \"conversion rates\", \"load time\", \"accuracy\", \"engagement\",\n",
        "        \"retention\", \"search results\", \"reporting speed\", \"onboarding\", \"data quality\"\n",
        "    ]\n",
        "    adverbs = [\n",
        "        \"notably\", \"marginally\", \"surprisingly\", \"rarely\", \"frequently\",\n",
        "        \"dramatically\", \"subtly\", \"steadily\", \"barely\", \"clearly\"\n",
        "    ]\n",
        "\n",
        "    # Words with a basic sentiment weight to influence the score a bit\n",
        "    pos_words = {\"improves\": 12, \"boosts\": 15, \"simplifies\": 10, \"enhances\": 12, \"reliably\": 8, \"dramatically\": 10, \"clearly\": 6, \"steadily\": 5}\n",
        "    neg_words = {\"breaks\": -18, \"harms\": -15, \"confuses\": -10, \"delays\": -8, \"barely\": -6}\n",
        "\n",
        "    texts, scores = [], []\n",
        "    for _ in range(n_rows):\n",
        "        s = random.choice(subjects)\n",
        "        v = random.choice(verbs)\n",
        "        o = random.choice(objects)\n",
        "        adv = random.choice(adverbs)\n",
        "\n",
        "        # Build a simple sentence\n",
        "        sentence = f\"{s} {v} {o} {adv}.\"\n",
        "        texts.append(sentence)\n",
        "\n",
        "        # Heuristic score: start around 60 with some noise, then nudge by word weights\n",
        "        base = 60 + np.random.normal(loc=0, scale=12)\n",
        "        tweak = 0\n",
        "        for w, wgt in pos_words.items():\n",
        "            if w in v or w in adv:\n",
        "                tweak += wgt\n",
        "        for w, wgt in neg_words.items():\n",
        "            if w in v or w in adv:\n",
        "                tweak += wgt\n",
        "\n",
        "        score = int(np.clip(base + tweak + np.random.normal(0, 6), 0, 100))\n",
        "        scores.append(score)\n",
        "\n",
        "    df = pd.DataFrame({\"text\": texts, \"score\": scores})\n",
        "\n",
        "    # Save to CSV (no index)\n",
        "    df.to_csv(out_path, index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = generate_fake_data(n_rows=300, seed=123, out_path=\"simulated_data.csv\")\n",
        "    print(df.head())\n",
        "    print(\"\\nSaved to simulated_data.csv\")\n",
        "\n",
        "# =====================================================\n",
        "# CONFIG FLAGS (read from environment variables)\n",
        "# -----------------------------------------------------\n",
        "# Use environment variables so that a user can turn\n",
        "# features on/off or scale them *without changing code*.\n",
        "#\n",
        "# ENABLE_DOMAIN_NUDGE = \"1\" (default) → enable URL-based nudging\n",
        "# ENABLE_DOMAIN_NUDGE = \"0\"           → disable URL-based nudging\n",
        "#\n",
        "# DOMAIN_NUDGE_SCALE = float (default 1.0) → multiplier to adjust\n",
        "#                                           how strong the domain nudges are.\n",
        "# =====================================================\n",
        "ENABLE_DOMAIN_NUDGE = os.getenv(\"ENABLE_DOMAIN_NUDGE\", \"1\") == \"1\"\n",
        "try:\n",
        "    DOMAIN_NUDGE_SCALE = float(os.getenv(\"DOMAIN_NUDGE_SCALE\", \"1.0\"))\n",
        "except ValueError:\n",
        "    DOMAIN_NUDGE_SCALE = 1.0  # safe fallback if env var is invalid\n",
        "\n",
        "# =====================================================\n",
        "# 1) RULE-BASED SCORER\n",
        "# -----------------------------------------------------\n",
        "# These dictionaries represent \"cue words\" that help or hurt credibility.\n",
        "# - POSITIVE_CUES: adds points when detected\n",
        "# - NEGATIVE_CUES: subtracts points when detected\n",
        "#\n",
        "# Rule-based scores are useful because they are transparent\n",
        "# and work even for unseen text (no ML training required).\n",
        "# =====================================================\n",
        "POSITIVE_CUES = {\n",
        "    \"significantly\": 6,\n",
        "    \"dramatically\": 6,\n",
        "    \"consistently\": 5,\n",
        "    \"reliably\": 5,\n",
        "    \"clearly\": 4,\n",
        "    \"evidence\": 5,\n",
        "    \"dataset\": 4,\n",
        "    \"replicated\": 6,\n",
        "    \"peer reviewed\": 8,\n",
        "    \"statistically significant\": 8,\n",
        "    \"benchmark\": 4,\n",
        "    \"confidence interval\": 8,\n",
        "    \"p<\": 8,\n",
        "    \"%\": 3,   # presence of percentages suggests quantitative evidence\n",
        "}\n",
        "\n",
        "NEGATIVE_CUES = {\n",
        "    \"breaks\": -12,\n",
        "    \"harms\": -10,\n",
        "    \"confuses\": -8,\n",
        "    \"delays\": -6,\n",
        "    \"barely\": -5,\n",
        "    \"maybe\": -6,\n",
        "    \"might\": -6,\n",
        "    \"could\": -5,\n",
        "    \"rumor\": -12,\n",
        "    \"clickbait\": -15,\n",
        "    \"unverified\": -12,\n",
        "    \"???\": -8,\n",
        "    \"!!!\": -6,\n",
        "}\n",
        "\n",
        "def rule_based_score(text: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute a credibility score using simple heuristics.\n",
        "    The score is clipped to [0, 100].\n",
        "\n",
        "    Logic:\n",
        "    - Start at 60 (optimistic baseline).\n",
        "    - Add/subtract points for positive/negative cue words.\n",
        "    - Add points if numbers are present.\n",
        "    - Subtract if many ALL-CAPS words (shouting).\n",
        "    - Subtract if excessive punctuation (!!!, ???).\n",
        "    - Penalize very short text; reward moderately detailed text.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 50.0  # neutral fallback for missing/invalid input\n",
        "\n",
        "    t = text.lower()\n",
        "    score = 60.0  # baseline\n",
        "\n",
        "    # Add/subtract for cue words\n",
        "    for cue, pts in POSITIVE_CUES.items():\n",
        "        if cue in t:\n",
        "            score += pts\n",
        "    for cue, pts in NEGATIVE_CUES.items():\n",
        "        if cue in t:\n",
        "            score += pts\n",
        "\n",
        "    # Reward presence of numbers\n",
        "    if re.search(r\"\\b\\d+(\\.\\d+)?\\b\", t):\n",
        "        score += 3\n",
        "\n",
        "    # Penalize many ALL-CAPS words\n",
        "    words = re.findall(r\"[A-Za-z]{3,}\", text)\n",
        "    caps_tokens = sum(1 for w in words if len(w) >= 3 and w.isupper())\n",
        "    score -= min(caps_tokens * 2.0, 10.0)\n",
        "\n",
        "    # Penalize excessive punctuation\n",
        "    exclam = text.count(\"!\")\n",
        "    qmarks = text.count(\"?\")\n",
        "    score -= min(exclam * 1.5 + qmarks * 1.0, 12.0)\n",
        "\n",
        "    # Length adjustment\n",
        "    n_tokens = len(re.findall(r\"\\w+\", t))\n",
        "    if n_tokens < 6:\n",
        "        score -= 6\n",
        "    elif 12 <= n_tokens <= 30:\n",
        "        score += 4\n",
        "\n",
        "    return float(np.clip(score, 0, 100))\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 2) LOAD DATASET\n",
        "# -----------------------------------------------------\n",
        "# Train the ML model on a sample CSV file.\n",
        "# File must have:\n",
        "# - a column called \"sentence\" or \"text\"\n",
        "# - a numeric column called \"score\"\n",
        "# =====================================================\n",
        "df = pd.read_csv(\"simulated_data.csv\")\n",
        "\n",
        "text_col = \"sentence\" if \"sentence\" in df.columns else \"text\"\n",
        "if text_col not in df.columns or \"score\" not in df.columns:\n",
        "    raise ValueError(\"CSV must have 'score' and either 'sentence' or 'text'.\")\n",
        "\n",
        "X_text = df[text_col].astype(str)\n",
        "y = df[\"score\"].astype(float)\n",
        "\n",
        "# =====================================================\n",
        "# 3) FEATURE ENGINEERING\n",
        "# -----------------------------------------------------\n",
        "# Combine:\n",
        "# - TF-IDF vector of words and bigrams\n",
        "# - Rule-based score as a numeric feature\n",
        "# =====================================================\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
        "X_tfidf = vectorizer.fit_transform(X_text)\n",
        "\n",
        "rule_scores = X_text.apply(rule_based_score).to_numpy().reshape(-1, 1)\n",
        "X_rules = csr_matrix(rule_scores)\n",
        "\n",
        "# Final feature matrix = [TF-IDF | rule_score]\n",
        "X = hstack([X_tfidf, X_rules], format=\"csr\")\n",
        "\n",
        "# =====================================================\n",
        "# 4) TRAIN MODEL\n",
        "# -----------------------------------------------------\n",
        "# Use Ridge regression:\n",
        "# - Handles high-dimensional sparse features well.\n",
        "# - Produces continuous output (0–100).\n",
        "# =====================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = Ridge(alpha=1.0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_ml = model.predict(X_test)\n",
        "\n",
        "# Rules-only baseline\n",
        "X_text_train, X_text_test = train_test_split(X_text, test_size=0.2, random_state=42)\n",
        "y_pred_rules = X_text_test.apply(rule_based_score).to_numpy()\n",
        "\n",
        "# Blended predictions = 70% ML + 30% Rules (tunable)\n",
        "BLEND_W = 0.7\n",
        "y_pred_blend = BLEND_W * y_pred_ml + (1 - BLEND_W) * y_pred_rules\n",
        "\n",
        "# Simple evaluation function\n",
        "def report(label, y_true, y_hat):\n",
        "    print(f\"{label}\")\n",
        "    print(\"  R²:\", round(r2_score(y_true, y_hat), 4))\n",
        "    print(\"  MSE:\", round(mean_squared_error(y_true, y_hat), 4))\n",
        "    print()\n",
        "\n",
        "y_test_array = y_test.to_numpy()\n",
        "report(\"ML only\", y_test_array, y_pred_ml)\n",
        "report(\"Rules only\", y_test_array, y_pred_rules)\n",
        "report(\"Blended\", y_test_array, y_pred_blend)\n",
        "\n",
        "# =====================================================\n",
        "# 5) CORE PREDICTOR\n",
        "# -----------------------------------------------------\n",
        "# Used inside the API to generate predictions for new inputs.\n",
        "# Returns ml_only, rules_only, blended predictions.\n",
        "# =====================================================\n",
        "def predict_sentences(sentences, blend_weight=0.7):\n",
        "    tfidf_feats = vectorizer.transform(sentences)\n",
        "    rule_feats = csr_matrix(np.array([rule_based_score(s) for s in sentences]).reshape(-1, 1))\n",
        "    X_new = hstack([tfidf_feats, rule_feats], format=\"csr\")\n",
        "\n",
        "    ml_pred = model.predict(X_new)\n",
        "    rule_pred = np.array([rule_based_score(s) for s in sentences])\n",
        "    blended = blend_weight * ml_pred + (1 - blend_weight) * rule_pred\n",
        "\n",
        "    return {\"ml_only\": ml_pred, \"rules_only\": rule_pred, \"blended\": blended}\n",
        "\n",
        "# =====================================================\n",
        "# 6) URL HELPERS + DOMAIN NUDGE\n",
        "# -----------------------------------------------------\n",
        "# Functions classify a URL’s domain and apply\n",
        "# a small credibility adjustment (e.g., +10 for .edu).\n",
        "#\n",
        "# Controlled by config flags:\n",
        "# - ENABLE_DOMAIN_NUDGE (on/off)\n",
        "# - DOMAIN_NUDGE_SCALE (scales strength)\n",
        "# =====================================================\n",
        "def _is_valid_url(url: str) -> bool:\n",
        "    \"\"\"Check if a string looks like a valid HTTP/HTTPS URL.\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        return bool(parsed.scheme in (\"http\", \"https\") and parsed.netloc)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _get_host(url: str) -> str:\n",
        "    try:\n",
        "        return urlparse(url).netloc.lower()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def _url_domain_category_and_points(url: str) -> tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Classify URL into a category and return (category, points).\n",
        "    Points will be scaled by DOMAIN_NUDGE_SCALE.\n",
        "    \"\"\"\n",
        "    if not url or not _is_valid_url(url):\n",
        "        return (\"invalid\", 0.0)\n",
        "\n",
        "    host = _get_host(url)\n",
        "\n",
        "    if any(s in host for s in (\".edu\", \".ac.\", \".gov\")):\n",
        "        return (\"academic_or_gov\", 10.0)\n",
        "\n",
        "    reputable_academic = (\n",
        "        \"nature.com\", \"science.org\", \"sciencedirect.com\", \"springer.com\",\n",
        "        \"wiley.com\", \"ieee.org\", \"acm.org\", \"nejm.org\", \"thelancet.com\",\n",
        "        \"jamanetwork.com\", \"royalsociety\", \"cambridge.org\", \"oup.com\",\n",
        "        \"cell.com\", \"pnas.org\"\n",
        "    )\n",
        "    if any(s in host for s in reputable_academic):\n",
        "        return (\"publisher_reputable\", 8.0)\n",
        "\n",
        "    if \"wikipedia.org\" in host:\n",
        "        return (\"wikipedia\", 5.0)\n",
        "\n",
        "    major_news = (\n",
        "        \"nytimes.com\", \"washingtonpost.com\", \"bbc.com\", \"reuters.com\",\n",
        "        \"apnews.com\", \"wsj.com\", \"ft.com\", \"economist.com\"\n",
        "    )\n",
        "    if any(s in host for s in major_news):\n",
        "        return (\"news_major\", 2.0)\n",
        "\n",
        "    if any(s in host for s in (\"medium.com\", \"substack.com\", \"blogspot.\", \"wordpress.\")):\n",
        "        return (\"platform_blog\", -5.0)\n",
        "\n",
        "    return (\"other\", 0.0)\n",
        "\n",
        "# =====================================================\n",
        "# 7) EXPLANATION BUILDER\n",
        "# -----------------------------------------------------\n",
        "# Generates a human-readable explanation string that\n",
        "# describes why a score was assigned.\n",
        "# =====================================================\n",
        "def _explain(text: str, ml_score: float, rule_score: float, blended: float, url: str | None = None) -> str:\n",
        "    t = (text or \"\").lower()\n",
        "    cues_pos = [c for c in POSITIVE_CUES if c in t][:3]\n",
        "    cues_neg = [c for c in NEGATIVE_CUES if c in t][:3]\n",
        "    has_num = bool(re.search(r\"\\b\\d+(\\.\\d+)?\\b\", t))\n",
        "    many_punct = (text.count(\"!\") + text.count(\"?\")) >= 3\n",
        "    caps_tokens = sum(1 for w in re.findall(r\"[A-Za-z]{3,}\", text) if w.isupper())\n",
        "\n",
        "    parts = []\n",
        "    parts.append(f\"blended={blended:.2f} (ml={ml_score:.2f}, rules={rule_score:.2f})\")\n",
        "    if cues_pos:\n",
        "        parts.append(\"pos cues: \" + \", \".join(cues_pos))\n",
        "    if cues_neg:\n",
        "        parts.append(\"neg cues: \" + \", \".join(cues_neg))\n",
        "    if has_num:\n",
        "        parts.append(\"numbers detected\")\n",
        "    if many_punct:\n",
        "        parts.append(\"excess punctuation\")\n",
        "    if caps_tokens >= 3:\n",
        "        parts.append(\"ALL-CAPS words\")\n",
        "\n",
        "    if url:\n",
        "        valid = _is_valid_url(url)\n",
        "        parts.append(\"url=\" + (\"valid\" if valid else \"invalid\"))\n",
        "        if valid:\n",
        "            cat, pts = _url_domain_category_and_points(url)\n",
        "            sign = \"+\" if pts >= 0 else \"\"\n",
        "            parts.append(f\"domain={cat}({sign}{int(pts * DOMAIN_NUDGE_SCALE)})\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "# =====================================================\n",
        "# 8) API FUNCTION\n",
        "# -----------------------------------------------------\n",
        "# score_api() validates inputs, calls predictors, and\n",
        "# returns a JSON-friendly object with:\n",
        "# - raw ML/rules/blended scores\n",
        "# - domain adjustments\n",
        "# - final score\n",
        "# - explanation string\n",
        "# - error list (if any)\n",
        "# =====================================================\n",
        "def score_api(payload: dict, *, blend_weight: float = 0.7) -> dict:\n",
        "    if not isinstance(payload, dict):\n",
        "        raise TypeError(\"payload must be a dict\")\n",
        "\n",
        "    items = payload.get(\"items\", [])\n",
        "    if not isinstance(items, list):\n",
        "        raise TypeError(\"payload['items'] must be a list\")\n",
        "\n",
        "    ids, texts, urls, errs = [], [], [], []\n",
        "    for idx, it in enumerate(items):\n",
        "        row_errs = []\n",
        "        if not isinstance(it, dict):\n",
        "            it = {}\n",
        "            row_errs.append(\"item_not_dict\")\n",
        "\n",
        "        # ID\n",
        "        _id = it.get(\"id\")\n",
        "        if not isinstance(_id, str):\n",
        "            _id = f\"row-{idx}\"\n",
        "        ids.append(_id)\n",
        "\n",
        "        # Text\n",
        "        txt = it.get(\"text\")\n",
        "        if not isinstance(txt, str) or not txt.strip():\n",
        "            row_errs.append(\"missing_or_invalid_text\")\n",
        "            txt = \"\"\n",
        "        texts.append(txt)\n",
        "\n",
        "        # URL\n",
        "        url = it.get(\"url\", \"\")\n",
        "        if not isinstance(url, str):\n",
        "            row_errs.append(\"invalid_url_type\")\n",
        "            url = \"\"\n",
        "        elif url and not _is_valid_url(url):\n",
        "            row_errs.append(\"malformed_url\")\n",
        "        urls.append(url)\n",
        "        errs.append(row_errs)\n",
        "\n",
        "    # Predictions from ML + rules\n",
        "    out = predict_sentences(texts, blend_weight=blend_weight)\n",
        "    ml, rb, bl = out[\"ml_only\"], out[\"rules_only\"], out[\"blended\"]\n",
        "\n",
        "    # Convert to safe floats\n",
        "    def _f(x):\n",
        "        try:\n",
        "            v = float(x)\n",
        "            if math.isnan(v) or math.isinf(v):\n",
        "                return float(\"nan\")\n",
        "            return v\n",
        "        except Exception:\n",
        "            return float(\"nan\")\n",
        "\n",
        "    results = []\n",
        "    for i in range(len(texts)):\n",
        "        ml_i, rb_i, bl_i = _f(ml[i]), _f(rb[i]), _f(bl[i])\n",
        "\n",
        "        # Apply domain nudge if enabled\n",
        "        url_cat = \"none\"\n",
        "        url_adj = 0.0\n",
        "        if ENABLE_DOMAIN_NUDGE and urls[i]:\n",
        "            cat, base_pts = _url_domain_category_and_points(urls[i])\n",
        "            url_cat = cat\n",
        "            url_adj = float(base_pts * DOMAIN_NUDGE_SCALE)\n",
        "\n",
        "        # Final score = blended + domain adjustment\n",
        "        final_score = float(np.clip(bl_i + url_adj, 0.0, 100.0))\n",
        "\n",
        "        results.append({\n",
        "            \"id\": ids[i],\n",
        "            \"text\": texts[i],\n",
        "            \"url\": urls[i],\n",
        "            \"scores\": {  # transparency: raw components\n",
        "                \"ml_only\": ml_i,\n",
        "                \"rules_only\": rb_i,\n",
        "                \"blended\": bl_i\n",
        "            },\n",
        "            \"url_category\": url_cat,\n",
        "            \"url_adjustment\": url_adj,\n",
        "            \"score\": final_score,  # preferred field\n",
        "            \"explanation\": _explain(texts[i], ml_i, rb_i, bl_i, urls[i]),\n",
        "            \"errors\": errs[i],\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"meta\": {\n",
        "            \"count\": len(results),\n",
        "            \"blend_weight\": float(blend_weight),\n",
        "            \"enable_domain_nudge\": ENABLE_DOMAIN_NUDGE,\n",
        "            \"domain_nudge_scale\": DOMAIN_NUDGE_SCALE,\n",
        "        }\n",
        "    }\n"
      ]
    }
  ]
}